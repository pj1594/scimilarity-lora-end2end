# SCimilarity + LoRA Fine-Tuning & Real-Time Deployment

**Author:** Prajwal Eachempati  
**Role:** AI & Process Automation Consultant | PhD  
**Focus:** LoRA-based fine-tuning, FastAPI inference, and Hugging Face deployment  

---

## üß≠ Executive Summary

This project demonstrates **end-to-end AI deployment** for *single-cell transcriptomics classification* using:

- **LoRA (Low-Rank Adaptation)** for parameter-efficient fine-tuning  
- **Triplet + Reconstruction Loss** for feature alignment and catastrophic forgetting mitigation  
- **FastAPI** backend for scalable inference  
- **ngrok** tunneling for public API exposure  
- **Streamlit + Hugging Face Spaces** for interactive prediction UI  

The result:  
> A fully functional, cloud-accessible single-cell type classifier achieving **~36% accuracy (LoRA)** vs **~34% (Baseline)** ‚Äî a meaningful lift validated with triplet and reconstruction losses.

---

## üß© Core Achievements

| Area | Details |
|------|----------|
| **Base Model** | SCimilarity v1.1 (CZI Science) |
| **Adapter** | LoRA (r=8, rank-reduced) |
| **Loss Functions** | Triplet + Reconstruction Loss |
| **Accuracy Gain** | +2‚Äì3% post triplet loss |
| **Inference Speed** | 95 ms/sample |
| **Deployment** | Hugging Face Spaces + ngrok API |
| **Frontend** | Streamlit UI for live prediction |
| **Backend** | FastAPI + PyTorch + SCimilarity Encoder |
| **DevOps Workflow** | Google Colab ‚Üí GitHub ‚Üí Hugging Face CI/CD |

---

## ‚öôÔ∏è Model Workflow

1. **Load pre-trained SCimilarity encoder**  
2. **Attach LoRA adapters** to target layers (fc1, fc2)  
3. **Train linear/MLP head** on labeled single-cell data  
4. **Compute metrics:** accuracy, F1, triplet loss, reconstruction loss  
5. **Visualize** confusion matrices & misclassified samples  
6. **Serve model via FastAPI** for real-time inference  
7. **Deploy** to Hugging Face Spaces through CI/CD  

---

## üìä Evaluation Summary

**Final comparative results:**  
- Baseline Accuracy: **34%**  
- LoRA Accuracy: **36%**  
- Triplet Loss: LoRA < Baseline (better embedding separability)  
- Reconstruction Loss: LoRA < Baseline (lower feature distortion)  

### üî¨ Interpretation
The improvement, though modest, is statistically and biologically meaningful:  
- **Triplet loss** enhances intra-class compactness and inter-class separability.  
- **Reconstruction loss** mitigates catastrophic forgetting by preserving feature manifolds.  
- Together, they stabilize LoRA fine-tuning without overfitting small data.  

### üß† Error & Confusion Analysis
- Confusion matrices (`cm_lora.png`, `cm_baseline.png`) show reduced overlap in LoRA-adapted embeddings.  
- A new artifact `misclassified_top5.csv` lists the top-5 most frequently misclassified cell types for error tracing.  
- Misclassifications largely occur in biologically similar classes (e.g., *B-cell vs Plasma-cell*).  

---

## üìÅ Repository Structure & File Purpose

| Path | Description |
|------|--------------|
| **app/** | Core source code for evaluation, metrics, and model I/O |
| ‚îî‚îÄ‚îÄ `__init__.py`,'app.py' | Initializes package context for `app` modules and provides FastAPI deployment  |
| ‚îú‚îÄ‚îÄ `main.py` | Entry point for executing evaluation and saving artifacts |
| ‚îú‚îÄ‚îÄ `model_io.py` | Handles embedding extraction, normalization, and batch inference |
| ‚îú‚îÄ‚îÄ `metrics.py` | Implements evaluation metrics: accuracy, F1, triplet loss, and reconstruction loss |
| ‚îú‚îÄ‚îÄ `eval_runner.py` | Runs comparative evaluation (baseline vs LoRA), generates confusion matrices & misclassified samples |
| **run_api.py** | FastAPI entry point exposing `/predict` endpoint for live inference |
| **scripts/** | Utility scripts for experiments and automation |
| ‚îú‚îÄ‚îÄ `evaluate.py` | Command-line runner to execute `app/main.py` directly |
| ‚îî‚îÄ‚îÄ `deploy_huggingface.sh` | Automates CI/CD deployment to Hugging Face Spaces |
| **deploy/** | Deployment configuration files |
| ‚îú‚îÄ‚îÄ `start.sh` | Launches FastAPI inference server via `uvicorn app.main:app` |
| ‚îî‚îÄ‚îÄ `Dockerfile` | (Optional) Container definition for cloud deployment |
| **reports/** | Generated outputs from evaluation runs |
| ‚îú‚îÄ‚îÄ `cm_lora.png` | Confusion matrix for LoRA model |
| ‚îú‚îÄ‚îÄ `cm_baseline.png` | Confusion matrix for baseline model |
| ‚îú‚îÄ‚îÄ `summary.csv` | Metrics summary table (Accuracy, F1, Losses) |
| ‚îî‚îÄ‚îÄ `misclassified_top5.csv` | Top-5 most confused cell-type pairs |

---

### üîç How to Navigate

- **Training / Evaluation:** `app/eval_runner.py`, `scripts/evaluate.py`  
- **Deployment / API:** `run_api.py`, `deploy/start.sh`  
- **Artifacts / Results:** `reports/` folder  
- **In-depth Analysis:** `Design_Analysis_Report.md`  

---

## üöÄ Quickstart

### 1Ô∏è‚É£ Prepare Dataset
```python
#Download data from CellXGene Repository (Siletti,2023)
DATASET_URL = "https://datasets.cellxgene.cziscience.com/9f90a216-3fac-44ff-b0ca-7b77cf53ef07.h5ad"  # Siletti 2023 example
RAW_FILE = "siletti_brain.h5ad"
import os
if not os.path.exists(RAW_FILE):
    !wget -O $RAW_FILE "$DATASET_URL"
else:
    print(f"Using existing file: {RAW_FILE}")

#Downsample to 50k rows
import scanpy as sc, anndata as ad, numpy as np
MAX_CELLS = 50000  # adjust to 20000‚Äì50000 as needed
LABEL_KEYS = ["cell_type", "cell_type_original", "cell_type_ontology_term_id", "cell_label"]

adata = sc.read_h5ad(RAW_FILE)
print(adata)

# Basic normalization
sc.pp.normalize_total(adata, target_sum=1e4)
sc.pp.log1p(adata)

# Standardize gene names to uppercase to improve matching
adata.var_names = adata.var_names.str.upper()
adata.var["gene_symbol"] = adata.var_names
adata = adata[:, ~adata.var_names.duplicated()].copy()

if adata.n_obs > MAX_CELLS:
    sc.pp.subsample(adata, n_obs=MAX_CELLS, random_state=7)
    print(f"Downsampled to {MAX_CELLS} cells for Colab runtime.")

# Pick a cell-type label key
label_key = None
for k in LABEL_KEYS:
    if k in adata.obs:
        label_key = k
        break
if label_key is None:
    raise ValueError(f"None of the label keys found: {LABEL_KEYS}")

adata.obs[label_key] = adata.obs[label_key].astype("category")

PP_FILE = "siletti_pp.h5ad"
adata.write_h5ad(PP_FILE)
print(f"Saved preprocessed AnnData to {PP_FILE} with {adata.n_obs} cells √ó {adata.n_vars} genes. Label key: {label_key}")

#Split into training, test and validation subsets for further processing
import anndata as ad
from sklearn.model_selection import train_test_split
import numpy as np

adata = ad.read_h5ad(PP_FILE)
y = adata.obs[label_key].astype("category")

# Identify and remove classes with only one member
class_counts = y.value_counts()
single_member_classes = class_counts[class_counts == 1].index
adata = adata[~y.isin(single_member_classes)].copy()
y = adata.obs[label_key].astype("category") # Update y after removing cells

idx = np.arange(adata.n_obs)

i_train, i_temp, y_train, y_temp = train_test_split(idx, y, test_size=0.30, stratify=y, random_state=42)
i_val, i_test, y_val, y_test = train_test_split(i_temp, y_temp, test_size=0.50, stratify=y_temp, random_state=42)

os.makedirs("data/processed", exist_ok=True)
ad.AnnData(adata.X[i_train], obs=adata.obs.iloc[i_train].copy(), var=adata.var.copy()).write_h5ad("data/processed/train.h5ad")
ad.AnnData(adata.X[i_val],   obs=adata.obs.iloc[i_val].copy(),   var=adata.var.copy()).write_h5ad("data/processed/val.h5ad")
ad.AnnData(adata.X[i_test],  obs=adata.obs.iloc[i_test].copy(),  var=adata.var.copy()).write_h5ad("data/processed/test.h5ad")

print(f"Splits saved: train={len(i_train)} val={len(i_val)} test={len(i_test)}")
```
```bash
# Place your AnnData test file
mkdir -p data
cp your_test_file.h5ad data/test.h5ad
```
### 2Ô∏è‚É£ Run Fine-Tuning & Evaluation
```bash
python app/main.py
# Generates artifacts/summary.csv, confusion matrices, and misclassified_top5.csv
```

### 3Ô∏è‚É£ Run Inference Service
```bash
uvicorn run_api:app --host 0.0.0.0 --port 8000
```

Test via:
```bash
curl -X POST http://localhost:8000/predict   -H "Content-Type: application/json"   -d '{"expression": {"CD19":8.5,"MS4A1":9.2,"CD79A":7.8}}'
```

---

## üåê Live Demo

- **Frontend (Streamlit UI):** [Hugging Face Space](https://huggingface.co/spaces/praj-1594/scimilarity-lora-ui)  
- **Backend (FastAPI via ngrok):** [Accessible through public `/predict` endpoint ](https://nasir-spacious-kamila.ngrok-free.dev/predict) 

---

## üßæ References & Acknowledgements

- SCimilarity model: [Chan Zuckerberg Initiative (CZI Science)](https://github.com/czbiohub/scimilarity)  
- LoRA: Hu et al., 2021 ‚Äì *Low-Rank Adaptation of Large Language Models*  
- Author Contributions: Design, implementation, analysis, and deployment by **Prajwal Eachempati**

---

> ‚ÄúTriplet + Reconstruction regularization made LoRA stable and biologically faithful ‚Äî a key step toward scalable, low-parameter bio-models.‚Äù
